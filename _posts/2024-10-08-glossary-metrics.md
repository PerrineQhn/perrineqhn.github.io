---
title: 'M√©triques D'√©valuation'
date: 2024-10-08
#modified: 
permalink: /machine-learning-glossary/concepts/metrics
toc: false
excerpt: "Concepts ML : mesures d'√©valuation."
header: 
  teaser: "blog/glossary/glossary.png"
tags:
  - ML
  - Glossary
author_profile: false
redirect_from: 
  - /posts/2024/10/glossary-metrics
sidebar:
  title: "Glossaire ML"
  nav: sidebar-glossary

---

{% include base_path %}

## M√©triques de Classification

### M√©triques simples

:mag: <span class='note'> Notes secondaires </span> : L'accent est mis sur la classification binaire, mais la plupart des scores peuvent √™tre g√©n√©ralis√©s au param√®tre multi-classes. Souvent, cela est r√©alis√© en consid√©rant uniquement la "classe correcte" et la "classe incorrecte" afin d'en faire une classification binaire, puis vous faites la moyenne (pond√©r√©e par la proportion d'observation dans la classe) le score de chaque classe.> 

* **TP** / **TN** / **FN** / **FP:** Mieux compris √† travers un $$2*2$$ [confusion matrix](#visual-metrics).

<div markdown="1">
![confusion matrix](/images/blog/glossary-old/confusion-matrix.png){:width='477px'}
</div>

- **Exactitude (Accuracy)** : Fraction des observations correctement class√©es.

  - $Acc = \frac{Vrais Positifs}{Total} = \frac{TP+FN}{TP+FN+TN+FP}$

  - üí° Intuition : En g√©n√©ral, √† quel point peut-on faire confiance aux pr√©dictions ?

  - üîß Pratique : √Ä utiliser si aucune classe n‚Äôest d√©s√©quilibr√©e et si le co√ªt des erreurs est le m√™me pour les deux types.

- **Pr√©cision (Precision)** : Fraction des pr√©dictions positives qui √©taient effectivement positives.

  - $Precision = \frac{TP}{Pr√©dictions Positives} = \frac{TP}{TP+FP}$

  - üí° Intuition : √Ä quel point peut-on faire confiance aux pr√©dictions positives ?

  - üîß Pratique : √Ä utiliser si les faux positifs (FP) sont les pires erreurs.

- **Rappel (Recall)** : Fraction des observations positives qui ont √©t√© correctement pr√©dites.
  - $ Rappel = \frac{TP}{Observations Positives} = \frac{TP}{TP+FN}$
  - üí° Intuition : Combien de vrais positifs allons-nous trouver ?
  - üîß Pratique : √Ä utiliser si les faux n√©gatifs (FN) sont les pires erreurs.

- **F1-Score** : Moyenne harmonique (utile pour la moyenne des taux) du rappel et de la pr√©cision.

  - $F1 = 2 \frac{Pr√©cision * Rappel}{Pr√©cision + Rappel}
    - Si le rappel est $\beta$ fois plus important que la pr√©cision, utilisez : $F_{\beta} = (1 + \beta^2)  \frac{Pr√©cision* Rappel}{\beta^2  Pr√©cision + Rappel}$

  - üí° Intuition : √Ä quel point pouvons-nous faire confiance √† nos algorithmes pour la classe positive ?

  - üîß Pratique : √Ä utiliser si la classe positive est la plus importante (par exemple, lorsque l‚Äôon cherche un d√©tecteur plut√¥t qu‚Äôun classificateur).

- Sp√©cificit√© (Specificity) : Rappel pour les classes n√©gatives.
  - $ Sp√©cificit√© = \frac{TN}{Observations N√©gatives} = \frac{TN}{TN+FP}$

- **Log-Loss** : Mesure la performance lorsque le mod√®le produit une probabilit√© \hat{y_{ic}} que l‚Äôobservation n appartienne √† la classe c.

  - Aussi appel√©e perte **d‚Äôentropie crois√©e** ou **perte logistique**.

  - LogLoss = - \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C y_{nc} \ln(\hat{y}_{nc})

  - Utilisez le logarithme naturel pour la coh√©rence.

  - Int√®gre l‚Äôid√©e de confiance probabiliste.

  - La Log-Loss est la m√©trique minimis√©e par la r√©gression logistique et plus g√©n√©ralement par le Softmax.

  - üí° **Intuition** : P√©nalise davantage si le mod√®le est confiant mais se trompe (voir graphique ci-dessous).

  - üí° **Log-loss** est l‚Äô[entropie crois√©e](/machine-learning-glossary/information/#cross-entropy) entre la distribution des vraies √©tiquettes et les pr√©dictions.

  - üîß **Pratique** : √Ä utiliser lorsque vous vous int√©ressez √† la confiance dans les r√©sultats.

  - Le graphique ci-dessous montre la log-loss en fonction de la confiance de l‚Äôalgorithme pour classer une observation dans la bonne cat√©gorie. Pour plusieurs observations, on calcule la log-loss de chacune, puis on en fait la moyenne.

<div markdown="1">
![log loss](/images/blog/glossary-old/log-loss.png){:width='477px'}
</div>

- **Kappa de Cohen** : Am√©lioration de votre classificateur compar√©e √† la simple supposition de la classe la plus probable.

  - \kappa = \frac{exactitude - \%{ClasseMax}}{1 - \%{ClasseMax}}

  - Souvent utilis√©e pour calculer la fiabilit√© inter-√©valuateurs (ex : 2 humains) : $\kappa = \frac{p_o- p_e}{1 - p_e}$ o√π  $p_o$  est l‚Äôaccord observ√© et  $p_e$  est l‚Äôaccord attendu par hasard.

  - $ \kappa \leq 1$  (si  $<0$ , le r√©sultat est inutile).

  - üí° Intuition : Am√©lioration de l‚Äôexactitude pond√©r√©e par le d√©s√©quilibre des classes.

  - üîß Pratique : √Ä utiliser lorsque le d√©s√©quilibre entre les classes est important et que toutes les classes sont d‚Äôimportance similaire.

- **AUC** (Area **U**nder the Curve) : R√©sume les courbes en une seule m√©trique.

  - Elle fait g√©n√©ralement r√©f√©rence √† la courbe [ROC](#visual-metrics), mais peut aussi √™tre utilis√©e pour d‚Äôautres courbes comme celle pr√©cision-rappel.
  - üí° Intuition : Probabilit√© qu‚Äôune observation positive s√©lectionn√©e al√©atoirement soit pr√©dite avec un score plus √©lev√© qu‚Äôune observation n√©gative s√©lectionn√©e al√©atoirement.
    - üí° AUC √©value les r√©sultats √† tous les points de coupure possibles. Cela permet d‚Äôobtenir de meilleures informations sur la capacit√© du classificateur √† s√©parer les classes. Cela la rend tr√®s diff√©rente des autres m√©triques qui d√©pendent g√©n√©ralement d‚Äôun seuil de coupure (par exemple, 0,5 pour la r√©gression logistique).
  - üîß Pratique : √Ä utiliser lors de la cr√©ation d‚Äôun classificateur pour des utilisateurs ayant des besoins diff√©rents (ils pourraient ajuster le point de coupure). De mon exp√©rience, l‚ÄôAUC est largement utilis√©e en statistique (~m√©trique de r√©f√©rence en biostatistiques), mais moins en machine learning.
    - Pr√©dictions al√©atoires :  $AUC = 0.5$. Pr√©dictions parfaites :  $AUC = 1$.

### M√©triques visuelles

- **Courbe ROC** : Receiver **O**perating Characteristic
- Graphique montrant le taux de vrais positifs (TP) par rapport au taux de faux positifs (FP), sur un seuil variable.
- Ce graphique de Wikip√©dia l‚Äôillustre bien :

<div markdown="1">
![ROC curve](/images/blog/glossary-old/ROC.png){:width='477px'}
</div>

- **Matrice de confusion** : Une matrice  $C*C$  qui montre le nombre d‚Äôobservations de la classe  $c$  ayant √©t√© √©tiquet√©es  $c', \ \forall c=1 \ldots C \text{ et  } c'=1\ldots C$.

  - :mag: ‚ÄãRemarque : Faites attention, les gens ne sont pas toujours coh√©rents avec les axes : vous pouvez trouver des matrices ‚Äúr√©el-pr√©dit‚Äù et ‚Äúpr√©vu-r√©el‚Äù.

  - Cela est mieux compris avec un exemple :

<div markdown="1">
![Multi Confusion Matrix](/images/blog/glossary-old/multi-confusion-matrix.png){:width='477px'}
</div>